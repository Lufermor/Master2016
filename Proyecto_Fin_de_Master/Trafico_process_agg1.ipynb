{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Tráfico Históricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este documento procesamos los datos históricos de tráfico (años 2015, 2016 y 2017(meses 01-02 y 03).\n",
    "Los pasos que seguimos son los siguientes:\n",
    "\n",
    "1) Crear una base de datos Mongo para almacenar los datos una vez procesados y enriquecidos.\n",
    "\n",
    "2) Cargar las coordenadas de cada estación (fichero __pmed_trafico_coord.csv__ proporcionado por Borja)\n",
    "\n",
    "3) Procesar de forma separada cada año\n",
    "\n",
    "* Los años 2015 y 2016 mantienen un formato común. Cada mes es un fichero CSV  comprimido con datos separados por ';'.\n",
    "* Los meses de Enero y Febrero de 2017 mantienen el mismo formato que 2015 y 2016. __El mes 03 de 2017 es un CSV comprimido con dato separados por ','__.\n",
    "* Fechas en formato datetime\n",
    "* Coordenadas en formato (x,y)\n",
    "* Eliminadas las medidiones de estaciones sin coordenadas\n",
    "\n",
    "4) Una vez procesados los años se almacenan en Mongo. \n",
    "    \n",
    "* La base de datos se llama *trafico*\n",
    "* Tres colecciones\n",
    "    * trafico2015 (111257705 documentos , 3 horas de proceso)\n",
    "    * trafico2016 (119050059 documentos , 3 horas de proceso)\n",
    "    * trafico2017 ( 28789567 documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pandas.io.json import json_normalize\n",
    "import zipfile\n",
    "import time\n",
    "# Mostrar la hora en formato HORAS:MINUTOS:SEGUNDOS\n",
    "print (time.strftime('%H:%M:%S'))\n",
    "import re\n",
    "from dateutil.parser import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MONGO\n",
    "def open_conection():\n",
    "    client = pymongo.MongoClient('localhost',27017)\n",
    "    return client\n",
    "\n",
    "def storeTrafico_ant(col, tabla):\n",
    "    d = tabla.to_json(orient='records', date_format = 'iso', date_unit = 's')\n",
    "    data_json = json.loads(d)\n",
    "    col.insert_many(data_json)   \n",
    "    \n",
    "def storeTrafico(col, tabla):\n",
    "    d = tabla.to_json(orient='records', date_format = 'iso')\n",
    "    data_json = json.loads(d)\n",
    "    #print(data_json[0])\n",
    "    for d in data_json:\n",
    "        d['fecha']= parse(d['fecha'])\n",
    "    col.insert_many(data_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# borrar la base de datos tráfico y crearse una nueva. Solo si no existe ya la BD trafico\n",
    "# Esto solo lo ejecuto una vez en la vida\n",
    "def crearDBTraffic(client):\n",
    "    client.drop_database('trafico_timestamp')\n",
    "    return True\n",
    "\n",
    "\"\"\"\n",
    "client = open_conection()\n",
    "crearDBTraffic(client)\n",
    "client.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = open_conection()\n",
    "db = client.trafico_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de funciones auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto al enriquecimiento, elimino todas aquellas entradas que corresponden con estaciones sin coordenadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rutas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "historico_2015 = \"./TRAFICO/historico/2015-20170427T093654Z-001/2015\"\n",
    "historico_2016 = \"./TRAFICO/historico/2016\"\n",
    "historico_2017 = \"./TRAFICO/historico/2017\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_files(path):\n",
    "    \"\"\"\n",
    "    input: directorio donde están los ficheros zip de un año. Uno por  mes\n",
    "    output: devuelve la lista de los ficheros .zip de cada mes\n",
    "    \"\"\"\n",
    "    return [ path +'/' + f for f in listdir(path) ]\n",
    "\n",
    "def df_month(file):\n",
    "    \"\"\" \n",
    "    input: fichero en formato csv\n",
    "    output: dataframe     \"\"\"\n",
    "    return pd.read_csv(file,  sep = ';', parse_dates =[1])\n",
    "\n",
    "def crear_csv(filemes, tabla):\n",
    "    tabla.to_csv(filemes, index = False)\n",
    "\n",
    "def df_resample(data):\n",
    "    fagg = { 'identif': min, 'tipo_elem': min, 'intensidad': np.mean,\n",
    "             'ocupacion': np.mean, 'carga': np.mean, 'vmed': np.mean,\n",
    "             'error': min, 'periodo_integracion' : np.mean}\n",
    "    \n",
    "    return data.resample('H', on='fecha').apply(fagg).dropna()\n",
    "\n",
    "def agghora(dfmonth):\n",
    "    return dfmonth.groupby(by =['idelem']).apply(df_resample)\n",
    "\n",
    "\n",
    "\n",
    "def punto_medida(x):\n",
    "    m30 = 'M-30'\n",
    "    urbanos = 'URBANOS'\n",
    "    res = re.search(m30, str(x))\n",
    "    if res:\n",
    "        return res.group(0)\n",
    "    else: \n",
    "        res = re.search(urbanos, str(x))\n",
    "        if res:\n",
    "            return res.group(0)\n",
    "        else: \n",
    "            return x\n",
    "    \n",
    "def process_year(zipfiles, coord, dbcol, modo = 'csv'):    \n",
    "    # creamos tantos dataframes como ficheros csv. Una tabla por mes\n",
    "    for filezip in zipfiles:     # 01-2015.zip\n",
    "        zf = zipfile.ZipFile(filezip, 'r')\n",
    "        csvfile = zf.namelist()[0]\n",
    "        month = df_month(zf.open(csvfile))  # dataframe de un año \n",
    "        month = agghora(month)              # agrego los datos por hora (vienen cada 15 minutos)\n",
    "        month.reset_index(inplace = True)\n",
    "        month.fecha = month.fecha + pd.to_timedelta(1, unit = 'h')\n",
    "        month.tipo_elem = month.tipo_elem.apply(lambda x :punto_medida(x))\n",
    "        month_coor = pd.merge(month, coord,  on = 'idelem' ) # con coordenadas\n",
    "        if modo != 'csv':\n",
    "        # llevarlo a json\n",
    "            storeTrafico(dbcol, month_coor)        \n",
    "        else: \n",
    "            # llevarlo a csv\n",
    "            crear_csv(\"./procesados/\" + csvfile, month_coor)\n",
    "        print('Mes: ', csvfile)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobar el árbol de ficheros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 2015\n",
    "#---------------------------------------------------------------------------\n",
    "# proceso de validación de los ficheros\n",
    "# comprobar que dentro de cada ZIP haya un fichero csv con el mismo nombre\n",
    "#---------------------------------------------------------------------------\n",
    "monthZIPfiles_2015 = extract_files(historico_2015)\n",
    "for filezip in monthZIPfiles_2015:     # 01-2015.zip\n",
    "    zf = zipfile.ZipFile(filezip, 'r')\n",
    "    print(zf.namelist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 2016\n",
    "#---------------------------------------------------------------------------\n",
    "# proceso de validación de los ficheros\n",
    "# comprobar que dentro de cada ZIP haya un fichero csv con el mismo nombre\n",
    "#---------------------------------------------------------------------------\n",
    "monthZIPfiles_2016 = extract_files(historico_2016)\n",
    "for filezip in monthZIPfiles_2016:     # 01-2015.zip\n",
    "    zf = zipfile.ZipFile(filezip, 'r')\n",
    "    print(zf.namelist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 2017 \n",
    "#---------------------------------------------------------------------------\n",
    "# proceso de validación de los ficheros\n",
    "# comprobar que dentro de cada ZIP haya un fichero csv con el mismo nombre\n",
    "#---------------------------------------------------------------------------\n",
    "monthZIPfiles_2017 = extract_files(historico_2017)\n",
    "for filezip in monthZIPfiles_2017:     # 01-2015.zip\n",
    "    zf = zipfile.ZipFile(filezip, 'r')\n",
    "    print(zf.namelist())\n",
    "    \n",
    "# Solo tenemos los datos hasta abril\n",
    "# Ojo: el mes de marzo viene separado por , en lugar de ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()\n",
    "Crear los csvs de todos los meses de cada año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if client:\n",
    "    print('Cierro conexion mongo')\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Abrir conexión mongo y accedemos a la bd Trafico\n",
    "client = open_conection()\n",
    "db = client.trafico_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# leemos el fichero de las coordenadas\n",
    "coordenadas = pd.read_csv('./pmed_trafico_coord.csv', sep = ';', decimal= b',',\n",
    "                         usecols = ['idelem', 'cod_cent', 'Xcoord', 'Ycoord'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Año 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 2015 Ejecución de todo el 2015\n",
    "print (time.strftime('%H:%M:%S'))\n",
    "col2015 = db.trafico2015\n",
    "monthZIPfiles_2015 = extract_files(historico_2015)\n",
    "b2015 = process_year(monthZIPfiles_2015, coordenadas, col2015, modo = 'csv')\n",
    "print (time.strftime('%H:%M:%S'))\n",
    "b2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col2015.count()    #número de json cargados en mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Año 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 2016 Ejecución de todo el 2016\n",
    "col2016 = db.trafico2016\n",
    "monthZIPfiles_2016 = extract_files(historico_2016)\n",
    "b2016 = process_year(monthZIPfiles_2016, coordenadas, col2016, modo = 'csv')\n",
    "b2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col2016.count()  #número de json cargados en mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Año 2017 Enero y Febrero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 2017 Ejecución de enero y febrero de 2017\n",
    "print (time.strftime('%H:%M:%S'))\n",
    "col2017 = db.trafico2017\n",
    "monthZIPfiles_2017 = extract_files(historico_2017)\n",
    "# proceso los dos primeros meses:\n",
    "b2017 = process_year(monthZIPfiles_2017[:2], coordenadas, col2017, modo = 'csv')\n",
    "\n",
    "print (time.strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Año 2017 Marzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ahora proceso el mes 3 que viene separado por ,\n",
    "print (time.strftime('%H:%M:%S'))\n",
    "col2017 = db.trafico20178\n",
    "col2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile(monthZIPfiles_2017[2], 'r')\n",
    "csvfile = zf.namelist()[0]\n",
    "month = pd.read_csv(zf.open(csvfile), sep =',', parse_dates =[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "month = agghora(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month.tipo_elem = month.tipo_elem.apply(lambda x :punto_medida(x))\n",
    "month_coor = pd.merge(month, coordenadas,  on = 'idelem' ) # con coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# almacenar en mongo\n",
    "storeTrafico(col2017, month_coor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# almacenar en csv\n",
    "crear_csv(\"./procesados/\" + csvfile, month_coor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer csvs y guardar en mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = open_conection()\n",
    "db = client.trafico\n",
    "client, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ruta_trafico_procesados = \"./procesados\"\n",
    "files = extract_files(ruta_trafico_procesados)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = db.trafico2015\n",
    "col.drop() # borro la colección entera\n",
    "for file in files:   \n",
    "    # vuelvo a mongo los de de 2017 \n",
    "    if  int(file[-8:-4]) != 2015 :\n",
    "        continue\n",
    "    col = db.trafico2015\n",
    "    df_traf = pd.read_csv(file, parse_dates = [1], encoding='latin-1')   \n",
    "    df_traf.fecha = df_traf.fecha.map(lambda x: pd.Timestamp(x))\n",
    "    storeTrafico(col, df_traf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_tuples(files):\n",
    "    month_files = files\n",
    "    cont = 0\n",
    "    for mesxlsx in month_files:\n",
    "        # juego solo con los de 2017 (hecho)\n",
    "        if  int(mesxlsx[-8:-4]) != 2017 :\n",
    "            continue\n",
    "        #juego solo con los de 2016\n",
    "        #if  int(mesxlsx[-8:-4]) != 2016 :\n",
    "        #    continue\n",
    "        # juego solo con los de 2015\n",
    "        \"\"\"        if  int(mesxlsx[-8:-4]) != 2015 :\n",
    "            continue\"\"\"\n",
    "        \n",
    "        month = pd.read_csv(mesxlsx,  encoding='latin-1', \n",
    "                              parse_dates = [4])\n",
    "        print(mesxlsx, len(month))\n",
    "        cont = cont + len(month) \n",
    "    print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "485px",
    "left": "0px",
    "right": "20px",
    "top": "121px",
    "width": "231px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
